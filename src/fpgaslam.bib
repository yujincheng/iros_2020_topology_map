@misc{Tech:2019360, 
  title = {{DNNDK User Guide - Xilinx}}, 
  url = {https://www.xilinx.com/support/documentation/user_guides/ug1327-dnndk-user-guide.pdf}, 
  read = {false}, 
  year = {2019}
}
@misc{KITTIGroundTruth, 
  title = {KITTI GroundTruth}, 
  url = {https://github.com/ZhangXiwuu/KITTI_GroundTruth}, 
  read = {false}, 
  year = {2019}
}
@article{Zhan:2018e92, 
  title = {{Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction}}, 
  author = {Zhan, Huangying and Garg, Ravi and Weerasekera, Chamara Saroj and Li, Kejie and Agarwal, Harsh and Reid, Ian}, 
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
  year = {2018}
}
@article{Li:2018ca8, 
  title = {{UnDeepVO: Monocular Visual Odometry Through Unsupervised Deep Learning}}, 
  author = {Li, Ruihao and Wang, Sen and Long, Zhiqiang and Gu, Dongbing}, 
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  abstract = {We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVo:one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVoby using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.}, 
  pages = {7286--7291}, 
  eissn = {2577-087X}, 
  doi = {10.1109/icra.2018.8461251}, 
  year = {2018}
}
@article{DeTone:20183cd, 
  title = {{Superpoint: Self-supervised interest point detection and description}}, 
  author = {DeTone, Daniel and Malisiewicz, Tomasz and Rabinovich, Andrew}, 
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops}, 
  year = {2018}
}
@article{Sarlin:20187ab, 
  title = {{Leveraging Deep Visual Descriptors for Hierarchical Efficient Localization}}, 
  author = {Sarlin, Paul-Edouard and Debraine, Frédéric and Dymczyk, Marcin and Siegwart, Roland and Cadena, Cesar}, 
  journal = {arXiv preprint arXiv:1809.01019}, 
  abstract = {Many robotics applications require precise pose estimates despite operating in large and changing environments. This can be addressed by visual localization, using a pre-computed 3D model of the surroundings. The pose estimation then amounts to finding correspondences between 2D keypoints in a query image and 3D points in the model using local descriptors. However, computational power is often limited on robotic platforms, making this task challenging in large-scale environments. Binary feature descriptors significantly speed up this 2D-3D matching, and have become popular in the robotics community, but also strongly impair the robustness to perceptual aliasing and changes in viewpoint, illumination and scene structure. In this work, we propose to leverage recent advances in deep learning to perform an efficient hierarchical localization. We first localize at the map level using learned image-wide global descriptors, and subsequently estimate a precise pose from 2D-3D matches computed in the candidate places only. This restricts the local search and thus allows to efficiently exploit powerful non-binary descriptors usually dismissed on resource-constrained devices. Our approach results in state-of-the-art localization performance while running in real-time on a popular mobile platform, enabling new prospects for robotics research.}, 
  arxiv = {1809.01019}, 
  year = {2018}
}
@article{Yu:2018c69, 
  title = {{Instruction Driven Cross-layer CNN Accelerator for Fast Detection on FPGA}}, 
  author = {Yu, Jincheng and Ge, Guangjun and Hu, Yiming and Ning, Xuefei and Qiu, Jiantao and Guo, Kaiyuan and Wang, Yu and Yang, Huazhong}, 
  journal = {ACM Transactions on Reconfigurable Technology and Systems (TRETS)}, 
  volume = {11}, 
  year = {2018}
}
@article{Brahmbhatt:2018c15, 
  title = {{Geometry-Aware Learning of Maps for Camera Localization}}, 
  author = {Brahmbhatt, Samarth and Gu, Jinwei and Kim, Kihwan and Hays, James and Kautz, Jan}, 
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
  year = {2018}
}
@article{Cieslewski:20187ee, 
  title = {{Data-Efficient Decentralized Visual SLAM}}, 
  author = {Cieslewski, Titus and Choudhary, Siddharth and Scaramuzza, Davide}, 
  journal = {2018 IEEE International Conference on Robotics and Automation (ICRA)}, 
  abstract = {Decentralized visual simultaneous localization and mapping (SLAM) is a powerful tool for multi-robot applications in environments where absolute positioning is not available. Being visual, it relies on cheap, lightweight and versatile cameras, and, being decentralized, it does not rely on communication to a central entity. In this work, we integrate state-of-the-art decentralized SLAM components into a new, complete decentralized visual SLAM system. To allow for data association and optimization, existing decentralized visual SLAM systems exchange the full map data among all robots, incurring large data transfers at a complexity that scales quadratically with the robot count. In contrast, our method performs efficient data association in two stages: first, a compact full-image descriptor is deterministically sent to only one robot. Then, only if the first stage succeeded, the data required for relative pose estimation is sent, again to only one robot. Thus, data association scales linearly with the robot count and uses highly compact place representations. For optimization, a state-of-the-art decentralized pose-graph optimization method is used. It exchanges a minimum amount of data which is linear with trajectory overlap. We characterize the resulting system and identify bottlenecks in its components. The system is evaluated on publicly available datasets and we provide open access to the code. Supplementary Material Data and code are at: https://github.com/uzh-rpg/dslam\_open}, 
  pages = {2466--2473}, 
  eissn = {2577-087X}, 
  doi = {10.1109/icra.2018.8461155}, 
  year = {2018}
}
@article{Golodetz:20184b8, 
  title = {{Collaborative Large-Scale Dense 3D Reconstruction with Online Inter-Agent Pose Optimisation}}, 
  author = {Golodetz, Stuart and Cavallari, Tommaso and Lord, Nicholas A. and Prisacariu, Victor A. and Murray, David W. and Torr, Philip H. S.}, 
  journal = {IEEE Transactions on Visualization and Computer Graphics}, 
  abstract = {Reconstructing dense, volumetric models of real-world 3D scenes is important for many tasks, but capturing large scenes can take significant time, and the risk of transient changes to the scene goes up as the capture time increases. These are good reasons to want instead to capture several smaller sub-scenes that can be joined to make the whole scene. Achieving this has traditionally been difficult: joining sub-scenes that may never have been viewed from the same angle requires a high-quality camera relocaliser that can cope with novel poses, and tracking drift in each sub-scene can prevent them from being joined to make a consistent overall scene. Recent advances, however, have significantly improved our ability to capture medium-sized sub-scenes with little to no tracking drift: real-time globally consistent reconstruction systems can close loops and re-integrate the scene surface on the fly, whilst new visual-inertial odometry approaches can significantly reduce tracking drift during live reconstruction. Moreover, high-quality regression forest-based relocalisers have recently been made more practical by the introduction of a method to allow them to be trained and used online. In this paper, we leverage these advances to present what to our knowledge is the first system to allow multiple users to collaborate interactively to reconstruct dense, voxel-based models of whole buildings using only consumer-grade hardware, a task that has traditionally been both time-consuming and dependent on the availability of specialised hardware. Using our system, an entire house or lab can be reconstructed in under half an hour and at a far lower cost than was previously possible.}, 
  volume = {24}, 
  pages = {2895--2905}, 
  issn = {1077-2626}, 
  eissn = {1077-2626}, 
  doi = {10.1109/tvcg.2018.2868533}, 
  year = {2018}
}
@article{Arandjelovic:2017997, 
  title = {{NetVLAD: CNN Architecture for Weakly Supervised Place Recognition}}, 
  author = {Arandjelovic, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef}, 
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following four principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the “Vector of Locally Aggregated Descriptors” image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we create a new weakly supervised ranking loss, which enables end-to-end learning of the architecture's parameters from images depicting the same places over time downloaded from Google Street View Time Machine. Third, we develop an efficient training procedure which can be applied on very large-scale weakly labelled tasks. Finally, we show that the proposed architecture and training procedure significantly outperform non-learnt image representations and off-the-shelf CNN descriptors on challenging place recognition and image retrieval benchmarks.}, 
  volume = {40}, 
  pages = {1437--1451}, 
  issn = {0162-8828}, 
  eissn = {1939-3539}, 
  doi = {10.1109/tpami.2017.2711011}, 
  year = {2017}
}
@article{Noh:2017d0b, 
  title = {{Large-scale image retrieval with attentive deep local features}}, 
  author = {Noh, Hyeonwoo and Araujo, Andre and Sim, Jack and Weyand, Tobias and Han, Bohyung}, 
  journal = {Proceedings of the IEEE International Conference on Computer Vision}, 
  year = {2017}
}
@article{Kasyanov:2017f17, 
  title = {{Keyframe-Based Visual-Inertial Online SLAM with Relocalization}}, 
  author = {Kasyanov, Anton and Engelmann, Francis and Stückler, Jörg and Leibe, Bastian}, 
  journal = {2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  abstract = {Complementing images with inertial measurements has become one of the most popular approaches to achieve highly accurate and robust real-time camera pose tracking. In this paper, we present a keyframe-based approach to visual-inertial simultaneous localization and mapping (SLAM) for monocular and stereo cameras. Our visual-inertial SLAM system is based on a real-time capable visual-inertial odometry method that provides locally consistent trajectory and map estimates. We achieve global consistency in the estimate through online loop-closing and non-linear optimization. Furthermore, our system supports relocalization in a map that has been previously obtained and allows for continued SLAM operation. We evaluate our approach in terms of accuracy, relocalization capability and run-time efficiency on public indoor benchmark datasets and on newly recorded outdoor sequences. We demonstrate state-of-the-art performance of our system compared to a visual-inertial odometry method and baseline visual SLAM approaches in recovering the trajectory of the camera.}, 
  pages = {6662--6669}, 
  doi = {10.1109/iros.2017.8206581}, 
  arxiv = {1702.02175}, 
  year = {2017}
}
@article{Balntas:20170a4, 
  title = {{HPatches: A benchmark and evaluation of handcrafted and learned local descriptors}}, 
  author = {Balntas, Vassileios and Lenc, Karel and Vedaldi, Andrea and Mikolajczyk, Krystian}, 
  journal = {Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  volume = {4}, 
  year = {2017}
}
@article{Cieslewski:2017c2d, 
  title = {{Efficient decentralized visual place recognition from full-image descriptors}}, 
  author = {Cieslewski, Titus and Scaramuzza, Davide}, 
  journal = {2017 International Symposium on Multi-Robot and Multi-Agent Systems (MRS)}, 
  abstract = {Visual multi-robot simultaneous localization and mapping (SLAM) is an effective way to provide state estimation to a group of robots that operate in an unstructured and GPS-denied environment. This is a problem that can be solved in a centralized way, but in some instances it can be desirable to solve it in a decentralized way. Decentralized visual place recognition, then, becomes a key component of a decentralized visual SLAM system. Achieving it by having all robots send queries to all other robots would use vast amounts of bandwidth, and diverse approaches have been explored by the robotics community to reduce that bandwidth. In previous work, we have proposed a decentralized version of bag-of-words place recognition, which, by using a distributed inverted index, is able to reduce bandwidth requirements by a factor of n, the robot count. In this short paper, we instead propose a decentralized visual place recognition method that is based on full-image descriptors. The method consists in clustering the full-image descriptor space into several clusters and assigning each cluster to one robot. As a result, place recognition can be achieved by sending each place query to only one robot. We evaluate the performance of our new method versus a centralized implementation using the Oxford Robotcar and KITTI datasets and explore an inherent trade-off between performance and load balancing.}, 
  pages = {78--82}, 
  doi = {10.1109/mrs.2017.8250934}, 
  arxiv = {1705.10739}, 
  year = {2017}
}
@article{Choudhary:2017e66, 
  title = {{Distributed mapping with privacy and communication constraints: Lightweight algorithms and object-based models}}, 
  author = {Choudhary, Siddharth and Carlone, Luca and Nieto, Carlos and Rogers, John and Christensen, Henrik I and Dellaert, Frank}, 
  journal = {The International Journal of Robotics Research}, 
  abstract = {We consider the following problem: a team of robots is deployed in an unknown environment and it has to collaboratively build a map of the area without a reliable infrastructure for communication. The backbone for modern mapping techniques is pose graph optimization, which estimates the trajectory of the robots, from which the map can be easily built. The first contribution of this paper is a set of distributed algorithms for pose graph optimization: rather than sending all sensor data to a remote sensor fusion server, the robots exchange very partial and noisy information to reach an agreement on the pose graph configuration. Our approach can be considered as a distributed implementation of a two-stage approach that already exists, where we use the Successive Over-Relaxation and the Jacobi Over-Relaxation as workhorses to split the computation among the robots. We also provide conditions under which the proposed distributed protocols converge to the solution of the centralized two-stage approach. As a second contribution, we extend the proposed distributed algorithms to work with the object-based map models. The use of object-based models avoids the exchange of raw sensor measurements (e.g. point clouds or RGB-D data) further reducing the communication burden. Our third contribution is an extensive experimental evaluation of the proposed techniques, including tests in realistic Gazebo simulations and field experiments in a military test facility. Abundant experimental evidence suggests that one of the proposed algorithms (the Distributed Gauss–Seidel method) has excellent performance. The Distributed Gauss–Seidel method requires minimal information exchange, has an anytime flavor, scales well to large teams (we demonstrate mapping with a team of 50 robots), is robust to noise, and is easy to implement. Our field tests show that the combined use of our distributed algorithms and object-based models reduces the communication requirements by several orders of magnitude and enables distributed mapping with large teams of robots in real-world problems. The source code is available for download at https://cognitiverobotics.github.io/distributed-mapper/}, 
  volume = {36}, 
  pages = {1286--1311}, 
  issn = {0278-3649}, 
  eissn = {1741-3176}, 
  doi = {10.1177/0278364917732640}, 
  year = {2017}
}
@article{Egodagamage:2017e3f, 
  title = {{A Collaborative Augmented Reality Framework Based on Distributed Visual Slam}}, 
  author = {Egodagamage, Ruwan and Tuceryan, Mihran}, 
  journal = {2017 International Conference on Cyberworlds (CW)}, 
  abstract = {Visual Simultaneous Localization and Mapping (SLAM) has been used for markerless tracking in augmented reality applications. Distributed SLAM helps multiple agents to collaboratively explore and build a global map of the environment while estimating their locations in it. One of the main challenges in Distributed SLAM is to identify local map overlaps of these agents, especially when their initial relative positions are not known. We developed a collaborative AR framework with freely moving agents having no knowledge of their initial relative positions. Each agent in our framework uses a camera as the only input device for its SLAM process. Furthermore, the framework identifies map overlaps of agents using an appearance-based method.}, 
  pages = {25--32}, 
  doi = {10.1109/cw.2017.47}, 
  year = {2017}
}
@article{Mur-Artal:2017281, 
  title = {{ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras}}, 
  author = {Mur-Artal, Ral and Tards, Juan D.}, 
  journal = {IEEE Transactions on Robotics}, 
  abstract = {We present ORB-SLAM2, a complete simultaneous localization and mapping (SLAM) system for monocular, stereo and RGB-D cameras, including map reuse, loop closing, and relocalization capabilities. The system works in real time on standard central processing units in a wide variety of environments from small hand-held indoors sequences, to drones flying in industrial environments and cars driving around a city. Our back-end, based on bundle adjustment with monocular and stereo observations, allows for accurate trajectory estimation with metric scale. Our system includes a lightweight localization mode that leverages visual odometry tracks for unmapped regions and matches with map points that allow for zero-drift localization. The evaluation on 29 popular public sequences shows that our method achieves state-of-the-art accuracy, being in most cases the most accurate SLAM solution. We publish the source code, not only for the benefit of the SLAM community, but with the aim of being an out-of-the-box SLAM solution for researchers in other fields.}, 
  volume = {33}, 
  pages = {1255--1262}, 
  issn = {1552-3098}, 
  eissn = {1941-0468}, 
  doi = {10.1109/tro.2017.2705103}, 
  arxiv = {1610.06475}, 
  year = {2016}
}
@article{Kendall:20150d9, 
  title = {{PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization}}, 
  author = {Kendall, Alex and Grimes, Matthew and Cipolla, Roberto}, 
  journal = {2015 IEEE International Conference on Computer Vision (ICCV)}, 
  abstract = {We present a robust and real-time monocular six degree of freedom relocalization system. Our system trains a convolutional neural network to regress the 6-DOF camera pose from a single RGB image in an end-to-end manner with no need of additional engineering or graph optimisation. The algorithm can operate indoors and outdoors in real time, taking 5ms per frame to compute. It obtains approximately 2m and 3° accuracy for large scale outdoor scenes and 0.5m and 5° accuracy indoors. This is achieved using an efficient 23 layer deep convnet, demonstrating that convnets can be used to solve complicated out of image plane regression problems. This was made possible by leveraging transfer learning from large scale classification data. We show that the PoseNet localizes from high level features and is robust to difficult lighting, motion blur and different camera intrinsics where point based SIFT registration fails. Furthermore we show how the pose feature that is produced generalizes to other scenes allowing us to regress pose with only a few dozen training examples.}, 
  pages = {2938--2946}, 
  doi = {10.1109/iccv.2015.336}, 
  year = {2015}
}
@article{Simonyan:20143be, 
  title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}}, 
  author = {Simonyan, Karen and Zisserman, Andrew}, 
  journal = {arXiv preprint arXiv:1409.1556}, 
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.}, 
  arxiv = {1409.1556}, 
  year = {2014}
}
@article{Galvez-Lopez:2012c94, 
  title = {{Bags of binary words for fast place recognition in image sequences}}, 
  author = {Gálvez-López, Dorian and Tardos, Juan D}, 
  journal = {IEEE Transactions on Robotics}, 
  volume = {28}, 
  year = {2012}
}
@article{Jegou:2010f45, 
  title = {{Aggregating local descriptors into a compact image representation}}, 
  author = {Jégou, Hervé and Douze, Matthijs and Schmid, Cordelia and Pérez, Patrick}, 
  journal = {CVPR 2010-23rd IEEE Conference on Computer Vision \& Pattern Recognition}, 
  year = {2010}
}
@article{Lowe:2004e6e, 
  title = {{Distinctive Image Features from Scale-Invariant Keypoints}}, 
  author = {Lowe, David G.}, 
  journal = {International Journal of Computer Vision}, 
  abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.}, 
  volume = {60}, 
  pages = {91--110}, 
  issn = {0920-5691}, 
  eissn = {1573-1405}, 
  doi = {10.1023/b:visi.0000029664.99615.94}, 
  year = {2004}
}
@book{bertsekas1989, 
  title = {{Parallel and distributed computation: numerical methods}}, 
  author = {Bertsekas, Dimitri P and Tsitsiklis, John N}, 
  read = {false}, 
  year = {1989}
}
@inproceedings{kendall2017end,
  title={End-to-end learning of geometry and context for deep stereo regression},
  author={Kendall, Alex and Martirosyan, Hayk and Dasgupta, Saumitro and Henry, Peter and Kennedy, Ryan and Bachrach, Abraham and Bry, Adam},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={66--75},
  year={2017}
}
# added by Feng Gao
@article{DBLP:journals/trob/Mur-ArtalMT15,
  author    = {Raul Mur{-}Artal and
               J. M. M. Montiel and
               Juan D. Tard{\'{o}}s},
  title     = {{ORB-SLAM:} {A} Versatile and Accurate Monocular {SLAM} System},
  journal   = {IEEE Transactions on Robotics},
  volume    = {31},
  number    = {5},
  pages     = {1147--1163},
  year      = {2015},
  url       = {https://doi.org/10.1109/TRO.2015.2463671},
  doi       = {10.1109/TRO.2015.2463671},
  timestamp = {Wed, 14 Nov 2018 00:00:00 +0100}
}
# added by Feng Gao
@article{Fang2017FPGAbasedOF,
  title={FPGA-based ORB feature extraction for real-time visual SLAM},
  author={Weikang Fang and Yanjun Zhang and Bo Yu and Shaoshan Liu},
  journal={2017 International Conference on Field Programmable Technology (ICFPT)},
  year={2017},
  pages={275-278}
}

# added by Feng Gao
@article{Yu:2018:IDC:3299999.3283452,
 author = {Yu, Jincheng and Ge, Guangjun and Hu, Yiming and Ning, Xuefei and Qiu, Jiantao and Guo, Kaiyuan and Wang, Yu and Yang, Huazhong},
 title = {Instruction Driven Cross-layer CNN Accelerator for Fast Detection on FPGA},
 journal = {ACM Trans. Reconfigurable Technol. Syst.},
 issue_date = {December 2018},
 volume = {11},
 number = {3},
 month = dec,
 year = {2018},
 issn = {1936-7406},
 pages = {22:1--22:23},
 articleno = {22},
 numpages = {23},
 url = {http://doi.acm.org/10.1145/3283452},
 doi = {10.1145/3283452},
 acmid = {3283452},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CNN, FPGA, image object detection},
} 

@article{Qiu:2016151, 
  title = {{Going deeper with embedded fpga platform for convolutional neural network}}, 
  author = {Qiu, Jiantao and Wang, Jie and Yao, Song and Guo, Kaiyuan and Li, Boxun and Zhou, Erjin and Yu, Jincheng and Tang, Tianqi and Xu, Ningyi and Song, Sen}, 
  journal = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays}, 
  year = {2016}
}

#KITTI
@article{geiger2013vision,
  title={Vision meets robotics: The KITTI dataset},
  author={Geiger, Andreas and Lenz, Philip and Stiller, Christoph and Urtasun, Raquel},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={11},
  pages={1231--1237},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}

@article{Tateno:2017776, 
  title = {{Cnn-slam: Real-time dense monocular slam with learned depth prediction}}, 
  author = {Tateno, Keisuke and Tombari, Federico and Laina, Iro and Navab, Nassir}, 
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, 
  year = {2017}
}

@inproceedings{liu2016ssd,
  title={Ssd: Single shot multibox detector},
  author={Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  booktitle={European conference on computer vision},
  pages={21--37},
  year={2016},
  organization={Springer}
}

@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3431--3440},
  year={2015}
}

@article{mao2018towards,
  title={Towards real-time object detection on embedded systems},
  author={Mao, Huizi and Yao, Song and Tang, Tianqi and Li, Boxun and Yao, Jun and Wang, Yu},
  journal={IEEE Transactions on Emerging Topics in Computing},
  volume={6},
  number={3},
  pages={417--431},
  year={2018},
  publisher={IEEE}
}

@article{zhang2018graph,
title={Graph-Based Place Recognition in Image Sequences with CNN Features},
author={Zhang, Xiwu and Wang, Lei and Zhao, Yan and Su, Yan},
journal={Journal of Intelligent \& Robotic Systems},
pages={1--15},
year={2018},
publisher={Springer}
}
@inproceedings{pizzoli2014remode,
  title={REMODE: Probabilistic, monocular dense reconstruction in real time},
  author={Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide},
  booktitle={2014 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={2609--2616},
  year={2014},
  organization={IEEE}
}